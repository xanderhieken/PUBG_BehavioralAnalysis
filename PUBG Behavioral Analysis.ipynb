{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlayerUnknown's Battlegrounds Behavioral Analysis\n",
    "### Xander Hieken\n",
    "\n",
    "Most of the cells in this notebook were reused many times throughout the project to make quick comparisons and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.stat import Correlation\n",
    "import pyarrow\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.pyplot import imread\n",
    "import matplotlib.markers\n",
    "from matplotlib.collections import PatchCollection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import PIL\n",
    "import mplcyberpunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Configuration\n",
    "warehouse_dir = 'DataWarehouse/'\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PUBG Data Warehouse\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"28g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseCompressedOops\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The following cell only needs to be run once to convert the CSV files into Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_to_add is a dictionary of file paths to be added to the data warehouse\n",
    "files_to_add = ['PUBGkills.csv', 'PUBGstats.csv']\n",
    "\n",
    "for i in files_to_add:\n",
    "    df = spark.read.load(i, format='csv', sep=',', inferSchema=True, header=True) # loads 2017 files\n",
    "    isolateTable = re.search('(?<=PUBG)(.*).csv', i) # isolates the string I want to use as the table name\n",
    "    tableName = isolateTable.group(1) # sets tableName to whatever is between 'PUBG' and '.csv' in the file path\n",
    "    df.write.saveAsTable(tableName, mode = 'overwrite') # saves the new tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Creating temporary views to work with in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over the course of this project, I kept coming back and adjusting things as needed, so this is not\n",
    "\n",
    "dfKills = spark.read.load(\"DataWarehouse/kills\")\n",
    "dfKills.select('killed_by','victim_name','victim_placement','map','victim_position_x','victim_position_y','match_id'\n",
    "              ).createOrReplaceTempView(\"kills\")\n",
    "\n",
    "dfStats = spark.read.load(\"DataWarehouse/stats\")\n",
    "dfStats.select('player_name', 'player_kills', 'player_assists', 'player_dmg', 'player_survive_time',\n",
    "               'team_placement', 'player_dbno','player_dist_ride', 'player_dist_walk','game_size', 'party_size',\n",
    "               'date', 'match_id').createOrReplaceTempView('stats')\n",
    "\n",
    "spark.sql(\"SELECT * FROM stats LEFT JOIN kills ON kills.match_id = stats.match_id \"\n",
    "          \"AND kills.victim_name = stats.player_name\").createOrReplaceTempView('combined')\n",
    "\n",
    "spark.catalog.dropTempView(\"kills\")\n",
    "spark.catalog.dropTempView(\"stats\")\n",
    "\n",
    "spark.sql(\"SELECT * \"\n",
    "          \"FROM combined \"\n",
    "          \"WHERE map = 'ERANGEL' \"\n",
    "          \"AND victim_position_x <= 819200 \"   \n",
    "          \"AND victim_position_y <= 819200 \"   \n",
    "          \"AND victim_position_x >= 0 \"\n",
    "          \"AND victim_position_y >= 0 \"\n",
    "          \"AND player_dist_walk + player_dist_ride > 10 \"\n",
    "          \"AND team_placement > 0 \"             \n",
    "          \"AND player_survive_time <= 2400 \"    \n",
    "          \"AND player_survive_time >= 120\"\n",
    "         ).createOrReplaceTempView('combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the combined data and splits it into separate views for each game mode\n",
    "\n",
    "spark.sql(\"SELECT * FROM combined WHERE party_size = 1\").createOrReplaceTempView('solos')\n",
    "spark.sql(\"SELECT * FROM combined WHERE party_size = 2\").createOrReplaceTempView('duos')\n",
    "spark.sql(\"SELECT * FROM combined WHERE party_size = 4\").createOrReplaceTempView('squads')\n",
    "spark.catalog.dropTempView('combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Generating Kill Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a minimal dataframe to plot all the kills/deaths on the Erangel map\n",
    "erDF = spark.sql(\"SELECT victim_position_x, victim_position_y FROM solos\")\n",
    "\n",
    "#Adjusting the coordinates to the resolution of the map image\n",
    "erDF = erDF.withColumn('victim_position_x', erDF.victim_position_x * 8192 / 813400) \\\n",
    "           .withColumn('victim_position_y', erDF.victim_position_y * 8192 / 813400) \\\n",
    "\n",
    "vx = np.array(erDF.select('victim_position_x').collect())\n",
    "vy = np.array(erDF.select('victim_position_y').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all kill/death locations on the map of Erangel\n",
    "bg = imread('pubg-match-deaths/erangel2.jpeg')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(82,82), dpi=100)\n",
    "ax.set_xlim(0, 8192); ax.set_ylim(0, 8192)\n",
    "ax.imshow(bg)\n",
    "\n",
    "plt.scatter(x=vx, y=vy, c='crimson', alpha=0.2, s=.2, marker='.')\n",
    "plt.scatter(x=vx1, y=vy1, c='crimson', alpha=0.2, s=.2, marker='.')\n",
    "plt.scatter(x=vx2, y=vy2, c='crimson', alpha=.2, s=.2, marker='.')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Getting counts of players that barely move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) \"\n",
    "          \"FROM solos \"\n",
    "          \"WHERE player_dist_walk + player_dist_ride = 0 \"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) \"\n",
    "          \"FROM solos \"\n",
    "          \"WHERE player_dist_walk + player_dist_ride <= 10 \"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) \"\n",
    "          \"FROM solos \"\n",
    "          \"WHERE player_dist_walk + player_dist_ride <= 100 \"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) \"\n",
    "          \"FROM solos \"\n",
    "          \"WHERE player_dist_walk + player_dist_ride <= 1000 \"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Plotting distance travelled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soloDF = np.array(spark.sql(\"SELECT player_dist_walk + player_dist_ride AS distance \"\n",
    "                            \"FROM solos \"\n",
    "                            \"WHERE team_placement <= 5 \"\n",
    "                           ).collect())\n",
    "\n",
    "duoDF = np.array(spark.sql(\"SELECT player_dist_walk + player_dist_ride AS distance \"\n",
    "                            \"FROM duos \"\n",
    "                            \"WHERE team_placement <= 5 \"\n",
    "                           ).collect())\n",
    "\n",
    "squadDF = np.array(spark.sql(\"SELECT player_dist_walk + player_dist_ride AS distance \"\n",
    "                            \"FROM squads \"\n",
    "                            \"WHERE team_placement <= 5 \"\n",
    "                           ).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('cyberpunk')  \n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "fig, ax = plt.subplots(figsize=(32,18))\n",
    "ax.set_xlim(0, 15000); ax.set_ylim(0, 21000)\n",
    "\n",
    "plt.xticks(np.arange(0, 15001, 500))\n",
    "plt.yticks(np.arange(0, 21001, 700))\n",
    "\n",
    "plt.hist(squadDF, bins=50000, alpha=1, color='#FE53BB', label='Squads')\n",
    "plt.hist(duoDF, bins=50000, alpha=.8, color='#00ff41', label='Duos')\n",
    "plt.hist(soloDF, bins=50000, alpha=.8, color='#08F7FE', label='Solos')\n",
    "\n",
    "plt.xlabel('Distance Traveled')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distance Traveled for Top 5 Placement')\n",
    "\n",
    "plt.legend(loc='upper right', frameon=False, fontsize='xx-large')\n",
    "\n",
    "#mplcyberpunk.add_glow_effects()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Plotting deaths over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squadDF = np.array(spark.sql(\"SELECT player_survive_time AS time FROM squads \"\n",
    "                             \"WHERE player_survive_time >=720\").collect())\n",
    "\n",
    "duoDF = np.array(spark.sql(\"SELECT player_survive_time AS time FROM duos \"\n",
    "                             \"WHERE player_survive_time >=720\").collect())\n",
    "\n",
    "soloDF = np.array(spark.sql(\"SELECT player_survive_time AS time FROM solos \"\n",
    "                             \"WHERE player_survive_time >=720\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('cyberpunk')  \n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "fig, ax = plt.subplots(figsize=(32,18))\n",
    "ax.set_xlim(0, 2220); ax.set_ylim(0, 480)\n",
    "\n",
    "plt.xticks(np.arange(0, 2221, 60)) \n",
    "plt.yticks(np.arange(0, 481, 20))\n",
    "\n",
    "#Vertical lines to show when the Bluezone reaches the circle and stops moving\n",
    "#plt.axvline(720, 0, 1, c='crimson', alpha=1) \n",
    "plt.axvline(1060, 0, 1, c='crimson', alpha=1)\n",
    "plt.axvline(1300, 0, 1, c='crimson', alpha=1)\n",
    "plt.axvline(1480, 0, 1, c='crimson', alpha=1)\n",
    "plt.axvline(1640, 0, 1, c='crimson', alpha=1)\n",
    "plt.axvline(1760, 0, 1, c='crimson', alpha=1)\n",
    "plt.axvline(1920, 0, 1, c='crimson', alpha=1)\n",
    "plt.axvline(2010, 0, 1, c='crimson', alpha=1)\n",
    "\n",
    "#Vertical lines to show when the Bluezone starts shrinking\n",
    "plt.axvline(420, 0, 1, c='dodgerblue', alpha=1)  \n",
    "plt.axvline(920, 0, 1, c='dodgerblue', alpha=1)\n",
    "plt.axvline(1210, 0, 1, c='dodgerblue', alpha=1)\n",
    "plt.axvline(1420, 0, 1, c='dodgerblue', alpha=1)\n",
    "plt.axvline(1600, 0, 1, c='dodgerblue', alpha=1)\n",
    "plt.axvline(1730, 0, 1, c='dodgerblue', alpha=1)\n",
    "plt.axvline(1850, 0, 1, c='dodgerblue', alpha=1)\n",
    "plt.axvline(1980, 0, 1, c='dodgerblue', alpha=1)\n",
    "\n",
    "#plt.axvline(120, 0, 1, c='F5D300', alpha=0.6) #1st circle shown\n",
    "#plt.axvline(2220, 0, 1, c='F5D300', alpha=0.6) #No more safezone\n",
    "\n",
    "\n",
    "plt.hist(squadDF, bins=100000, alpha=1, color='#FE53BB', label='Squads')\n",
    "plt.hist(duoDF, bins=100000, alpha=.8, color='#00ff41', label='Duos')\n",
    "plt.hist(soloDF, bins=100000, alpha=.8, color='#08F7FE', label='Solos')\n",
    "\n",
    "plt.xlabel('Time of Death (seconds from start of the game)') \n",
    "plt.ylabel('Count')\n",
    "plt.title('Deaths Over Time (Starting After First Circle)')\n",
    "plt.legend(loc='upper left', frameon=False, fontsize='xx-large')\n",
    "mplcyberpunk.add_glow_effects()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Random SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT killed_by, count(*) AS count \"\n",
    "          \"FROM squads \"\n",
    "          \"GROUP BY killed_by \"\n",
    "          \"ORDER BY count DESC\").show(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
